

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>jupyter: jupytext: formats: ipynb,md text_representation: extension: .md format_name: markdown format_version: ‘1.2’ jupytext_version: 1.7.1 kernelspec: display_name: Python 3 language: python name: python3 &mdash; The Multi-Mission Maximum Likelihood framework  documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^0.21.0-alpha.0/dist/embed-amd.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <link rel="stylesheet" href="../../_static/pygments/default.css" type="text/css" id="pygments-style">
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css">
   

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> The Multi-Mission Maximum Likelihood framework
          

          
            
            <img src="../../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intro.html">Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/logging.html">Logging and Verbosity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../xspec_users.html">Notes for XSPEC Users</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/Bayesian_tutorial.html">Bayesian Posterior Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modeling.html">Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/API.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/API.html#threeml">threeML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release_notes.html">Release Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Features and examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/Analysis_results_showcase.html">Analysis Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/random_variates.html">Random Variates</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/Point_source_plotting.html">Point source plotting basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/Building_Plugins_from_TimeSeries.html">Constructing plugins from TimeSeries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/grb080916C.html">Analyzing GRB 080916C</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/joint_BAT_gbm_demo.html">Example joint fit between GBM and Swift BAT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/joint_fitting_xrt_and_gbm_xspec_models.html">Joint fitting XRT and GBM data with XSPEC models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/flux_examples.html">Point Source Fluxes and Multiple Sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/Time-energy-fit.html">Time-energy fit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/synthetic_spectra.html">Generating Synthetic Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/gof_lrt.html">Goodness of Fit and Model Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/APEC_doc.html">Fitting XMM-Newton data with the APEC model</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">The Multi-Mission Maximum Likelihood framework</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
  <!--
  <div class="admonition note">
    <p><a href="https://github.com/lukelbd">ProPlot author here</a>: Due to my day job as a graduate student, certain <a href="https://github.com/lukelbd/proplot/pulls?q=is%3Aopen+is%3Apr">feature additions</a> may be delayed to the summer of 2020. In the meantime, if you are interested in contributing to ProPlot, please see the <a href="https://proplot.readthedocs.io/en/latest/contributions.html">contribution guide</a>. Any amount of help is welcome!
    </p>
  </div>
  -->
  <li id="lightdark-li">
    <label for="lightdark-checkbox" id="lightdark-label">
      <input type="checkbox" id="lightdark-checkbox"/>
      <div class="btn-neutral"></div>
    </label>
  </li>
  
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>jupyter:
jupytext:
formats: ipynb,md
text_representation:
extension: .md
format_name: markdown
format_version: ‘1.2’
jupytext_version: 1.7.1
kernelspec:
display_name: Python 3
language: python
name: python3</li>
    

    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
  <script type="text/javascript" src=../../_static/custom.js></script>
  
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<hr class="docutils" />
<div class="section" id="jupyter-jupytext-formats-ipynb-md-text-representation-extension-md-format-name-markdown-format-version-1-2-jupytext-version-1-7-1-kernelspec-display-name-python-3-language-python-name-python3">
<h1>jupyter:
jupytext:
formats: ipynb,md
text_representation:
extension: .md
format_name: markdown
format_version: ‘1.2’
jupytext_version: 1.7.1
kernelspec:
display_name: Python 3
language: python
name: python3<a class="headerlink" href="#jupyter-jupytext-formats-ipynb-md-text-representation-extension-md-format-name-markdown-format-version-1-2-jupytext-version-1-7-1-kernelspec-display-name-python-3-language-python-name-python3" title="Permalink to this headline">¶</a></h1>
<!-- #region --></div>
<div class="section" id="bayesian-posterior-sampling">
<h1>Bayesian Posterior Sampling<a class="headerlink" href="#bayesian-posterior-sampling" title="Permalink to this headline">¶</a></h1>
<p>When using Bayesian posterior sampling, we want to the posterior, ${\rm P}(M(\vec{\theta}) \vert D)$, of the model ($M(\vec{\theta})$)  given one or more datasets ($D$) (i.e., plugin instances) and one model containing one or more sources with free parameters $\vec{\theta} $ given a likelihood $L(\vec{\theta}) = {\rm P}(D \vert M(\vec{\theta}))$ and prior ${\rm P}(\theta)$ on the parameters. Only the simplest posteriors allow for an analytic solution, thus we must rely on Monte Carlo or nested sampling methods to sample the posterior.</p>
<p>In 3ML, we currently provide two popular posterior sampling methods: <a class="reference external" href="https://github.com/dfm/emcee">emcee</a> and <a class="reference external" href="https://github.com/farhanferoz/MultiNest">MULTINEST</a>. If you installed via conda, both packages are available, otherwise, only emcee is included.</p>
<div class="section" id="emcee">
<h2>emcee<a class="headerlink" href="#emcee" title="Permalink to this headline">¶</a></h2>
<p>“<a class="reference external" href="https://github.com/dfm/emcee">emcee</a> is an extensible, pure-Python implementation of Goodman &amp; Weare’s Affine Invariant Markov chain Monte Carlo (MCMC) Ensemble sampler.” It uses multiple “walkers” to explore the parameter space of the posterior. For a complete understanding of the capabilites and limitations, we recommend a thorough reading of <a class="reference external" href="http://msp.org/camcos/2010/5-1/p04.xhtml">Goodman &amp; Weare (2010)</a>. Nevertheless we emphasize these N points to keep in mind:</p>
<ul class="simple">
<li><p>emcee is for unimodal parameter estimation</p></li>
<li><p>for complex likelihoods and marginal likelihood integration, check out thermodynamic integration (documentation coming soon)</p></li>
</ul>
<p>Let’s take a look at its usage for a simple likelihood.</p>
<!-- #endregion --><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">seterr</span><span class="p">(</span><span class="nb">all</span><span class="o">=</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">capture</span>
<span class="kn">from</span> <span class="nn">threeML</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">threeML.bayesian.tutorial_material</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jupyterthemes</span> <span class="kn">import</span> <span class="n">jtplot</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">jtplot</span><span class="o">.</span><span class="n">style</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">fscale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">grid</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">silence_warnings</span><span class="p">()</span>
<span class="n">set_threeML_style</span><span class="p">()</span>

</pre></div>
</div>
<p>Let’s get a BayesianAnalysis object like the one we would have in a normal 3ML analysis. We use a custom function, prepared for this tutorial, which gives a BayesianAnalysis object having a very simple model with one free parameter ($\mu$), and with a likelihood having a very simple shape:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># This returns a BayesianAnalysis object with a simple likelihood function,</span>
<span class="c1"># and the corresponding Model instance. These objects are what you will have</span>
<span class="c1"># in a typical 3ML analysis. The Model contains one point source, named &quot;test&quot;,</span>
<span class="c1"># with a spectrum called &quot;simple&quot;</span>
<span class="n">bayes</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">get_bayesian_analysis_object_simple_likelihood</span><span class="p">()</span>
<span class="n">bayes</span><span class="o">.</span><span class="n">set_sampler</span><span class="p">(</span><span class="s1">&#39;emcee&#39;</span><span class="p">)</span>
<span class="c1"># Let&#39;s look at the likelihood function, which in this illustrative example</span>
<span class="c1"># has a very simple shape</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plot_likelihood_function</span><span class="p">(</span><span class="n">bayes</span><span class="p">)</span>
</pre></div>
</div>
<p>We must define a prior fo $\mu$ in order to sample. Let’s use a uniform prior from 0 to 100. There are two ways to define this in 3ML:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># This directly assigns the prior</span>
<span class="n">model</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">spectrum</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">Uniform_prior</span><span class="p">(</span><span class="n">lower_bound</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">upper_bound</span><span class="o">=</span><span class="mf">100.</span><span class="p">)</span>

<span class="c1"># Alternatively, we can set an uniformative prior on the parameter&#39;s bounds</span>
<span class="n">model</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">spectrum</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">set_uninformative_prior</span><span class="p">(</span><span class="n">Uniform_prior</span><span class="p">)</span>
</pre></div>
</div>
<p>Ok, we are ready to sample. We will use only 5 walkers for this problem. We need to define a burn_in sample length which is how long we want to run the sampler to approach the target distribution so that we do not include these “learning” samples in our final results. Additionally, we need to declare the number of samples each walker will take.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_walkers</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">burn_in</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">bayes</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">n_iterations</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span><span class="n">n_burn_in</span><span class="o">=</span><span class="n">burn_in</span><span class="p">,</span><span class="n">n_walkers</span><span class="o">=</span><span class="n">n_walkers</span> <span class="p">)</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">plot_sample_path</span><span class="p">(</span><span class="n">bayes</span><span class="p">,</span><span class="n">truth</span><span class="o">=</span><span class="mf">40.</span><span class="p">,</span><span class="n">burn_in</span><span class="o">=</span><span class="n">n_walkers</span><span class="o">*</span><span class="n">burn_in</span><span class="p">)</span>
</pre></div>
</div>
<p>We can see that we did not burn in the sampler long enough and we will have part of the burn in included in the results as can be seen in the marginal distribution of $\mu$:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">corner_plot</span><span class="p">()</span>
</pre></div>
</div>
<p>We could easily run the sampler longer, but let’s reset the value of $\mu$ far from the target distribution and try more burn in samples.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">spectrum</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span><span class="mi">99</span>


<span class="n">n_walkers</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">burn_in</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">plot_sample_path</span><span class="p">(</span><span class="n">bayes</span><span class="p">,</span><span class="n">truth</span><span class="o">=</span><span class="mf">40.</span><span class="p">,</span><span class="n">burn_in</span><span class="o">=</span><span class="n">n_walkers</span><span class="o">*</span><span class="n">burn_in</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">corner_plot</span><span class="p">()</span>
</pre></div>
</div>
<p>The marginal distribution of the parameter $\mu$ <em>is</em> the answer for our parameter. It is the integral of the posterior over all other parameters, $\vec{\phi}$, given the data.</p>
<p>$${\rm P}(\mu \vert D) = \int_{-\infty}^{\infty} {\rm d}\vec{\phi} ; {\rm P}( \mu, \vec{\phi}\vert D )$$</p>
<p>Of course, in our simple model, there are no other parameters. The marginal natuarally includes the dependence of all other parameters and is only equivalent to MLE error estimates under assumptions of Gaussianity and linearity.</p>
<p>We can sometimes use point-descriptors for the marginal, e.g., mean, median, etc., but when the marginal distribution is not easily symmetric, it can be more descriptive to plot it or speak of its highest posterior density interval:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">this_mu</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">get_variates</span><span class="p">(</span><span class="s1">&#39;test.spectrum.main.Simple.mu&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">this_mu</span><span class="o">.</span><span class="n">highest_posterior_density_interval</span><span class="p">(</span><span class="n">cl</span><span class="o">=</span><span class="mf">.68</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">this_mu</span><span class="o">.</span><span class="n">highest_posterior_density_interval</span><span class="p">(</span><span class="n">cl</span><span class="o">=</span><span class="mf">.95</span><span class="p">))</span>
</pre></div>
</div>
<div class="section" id="priors">
<h3>Priors<a class="headerlink" href="#priors" title="Permalink to this headline">¶</a></h3>
<p>We have not paid much attention to our prior choice. So far, we have used an uninformative prior (except that we know in our example the range over which the parameter can be found), but what happens if we limit our prior such that we would never find the target distribution?</p>
<p>Let’s set a uniform prior such that $\mu \in {80-100 }$.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">spectrum</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span><span class="mi">99</span>
<span class="n">model</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">spectrum</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">Uniform_prior</span><span class="p">(</span><span class="n">lower_bound</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">upper_bound</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">n_walkers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">burn_in</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">bayes</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">n_iterations</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span><span class="n">n_burn_in</span><span class="o">=</span><span class="n">burn_in</span><span class="p">,</span><span class="n">n_walkers</span><span class="o">=</span><span class="n">n_walkers</span> <span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">plot_sample_path</span><span class="p">(</span><span class="n">bayes</span><span class="p">,</span><span class="n">truth</span><span class="o">=</span><span class="mf">40.</span><span class="p">,</span><span class="n">burn_in</span><span class="o">=</span><span class="n">n_walkers</span><span class="o">*</span><span class="n">burn_in</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">corner_plot</span><span class="p">()</span>
</pre></div>
</div>
<p>We see that the over this range, the marginal distribution is the same as the prior, i.e., uniformative. The “data” have not updated our information about $\mu$ and thus we cannot conclude anything. This is by design in our example, but if we were to believe that $\mu$ had to be found in the interval $\mu \in{80-100}$, the data have not helped us to determine anything.</p>
<p>Let’s now look at setting a more informative prior on $\mu$. Suppose from either other measurements or physical insight, we are sure $\mu\sim 30$ with some uncertainty. Then we can impose a Gaussian prior over $\mu$.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">spectrum</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span><span class="mi">99</span>
<span class="n">model</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">spectrum</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">spectrum</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span><span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_walkers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">burn_in</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">bayes</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">n_iterations</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span><span class="n">n_burn_in</span><span class="o">=</span><span class="n">burn_in</span><span class="p">,</span><span class="n">n_walkers</span><span class="o">=</span><span class="n">n_walkers</span> <span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
<p>Our prior information has biased the results to slightly lower values of $\mu$. Again, this is only to illustrate how to use different priors and their effects on the recovered results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">plot_sample_path</span><span class="p">(</span><span class="n">bayes</span><span class="p">,</span><span class="n">truth</span><span class="o">=</span><span class="mf">40.</span><span class="p">,</span><span class="n">burn_in</span><span class="o">=</span><span class="n">n_walkers</span><span class="o">*</span><span class="n">burn_in</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">corner_plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="complex-likelihood">
<h2>Complex Likelihood<a class="headerlink" href="#complex-likelihood" title="Permalink to this headline">¶</a></h2>
<p>We now examine a more complex likelihood.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bayes</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">get_bayesian_analysis_object_complex_likelihood</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="multinest">
<h2>MULTINEST<a class="headerlink" href="#multinest" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://github.com/farhanferoz/MultiNest">MULTINEST</a> is a nested sampling algorithm that is designed to integrate the the posterior to obtain the marginal likelihood. For details on the algorithm see <a class="reference external" href="https://academic.oup.com/mnras/article-abstract/398/4/1601/981502">Feroz et al. (2009)</a>, <a class="reference external" href="https://arxiv.org/abs/1306.2144">Feroz et al. (2013)</a>, and for details on the input arguments for the python interface we implement, see the documentation of <a class="reference external" href="https://johannesbuchner.github.io/PyMultiNest/">pymultinest</a>. If you find these algorithms useful for your research, please cite the originals authors!</p>
<p>Let’s sample the complex likelihood from above with MULTINEST using</p>
<!-- #endregion --><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="n">bayes</span><span class="o">.</span><span class="n">set_sampler</span><span class="p">(</span><span class="s1">&#39;multinest&#39;</span><span class="p">)</span>


<span class="n">model</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">spectrum</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">Uniform_prior</span><span class="p">(</span><span class="n">lower_bound</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">upper_bound</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">plot_likelihood_function</span><span class="p">(</span><span class="n">bayes</span><span class="p">)</span>


<span class="n">bayes</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">n_live_points</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">resume</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="n">res</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">plot_sample_path</span><span class="p">(</span><span class="n">bayes</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">corner_plot</span><span class="p">()</span>
</pre></div>
</div>
<p>MULTINEST has fully sampled the likelihood and broken apart the modes. Cleary point-descriptors of the results will be inefficent, but we use the output files of MULTINEST to do multi-modal analysis. For details, consult the MULTINEST documentation.</p>
</div>
<div class="section" id="ultranest">
<h2>UltraNest<a class="headerlink" href="#ultranest" title="Permalink to this headline">¶</a></h2>
<p>Finally we can try with <a class="reference external" href="https://johannesbuchner.github.io/UltraNest/">ultranest</a> which is another nested sampling algorithm.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bayes</span><span class="o">.</span><span class="n">set_sampler</span><span class="p">(</span><span class="s1">&#39;ultranest&#39;</span><span class="p">)</span>
<span class="n">bayes</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">setup</span><span class="p">()</span>


<span class="n">res</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">quiet</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">plot_sample_path</span><span class="p">(</span><span class="n">bayes</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">bayes</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">corner_plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="accessing-samples-and-error-propagation">
<h2>Accessing samples and error propagation<a class="headerlink" href="#accessing-samples-and-error-propagation" title="Permalink to this headline">¶</a></h2>
<p>Error propagation with posterior samples is straight forward. Say we have a function $f(\mu) = Sin(\mu)$. For the complex likelihood, we would be out of luck if we were using MLE. However, we can directly calculate $f(\mu)$ with the samples from the posterior.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">mu</span><span class="p">):</span>
    
    <span class="k">return</span> <span class="n">mu</span><span class="o">**</span><span class="mi">2</span>
    
<span class="n">this_mu</span> <span class="o">=</span>  <span class="n">bayes</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">get_variates</span><span class="p">(</span><span class="s1">&#39;test.spectrum.main.Complex.mu&#39;</span><span class="p">)</span>
    
<span class="n">f_mu</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">this_mu</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="n">f_mu</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">f_mu</span><span class="o">.</span><span class="n">samples</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;f($\mu$)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>As with any approach, the Bayesian capabilities of 3ML are a tool. Understanding and proper use of the posterior and marginal distributions requires special care. For further reading on the Bayesian analysis and its applications to spectral analysis check out the following resources:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://betanalpha.github.io/writing/">Michael Betancourt</a> spends a ton of time trying to teach proper statistical methodology to applied fields. If you are using Bayesian techniques, you must read his work.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1411.5018">Frequentism and bayesianism: a python-driven primer</a></p></li>
<li><p><a class="reference external" href="http://www.tandfonline.com/doi/abs/10.1080/00107510802066753">Bayes in the sky: Bayesian inference and model selection in cosmology</a></p></li>
<li><p><a class="reference external" href="https://www.aanda.org/articles/aa/full_html/2014/04/aa22971-13/aa22971-13.html">X-ray spectral modelling of the AGN obscuring region in the CDFS: Bayesian model selection and catalogue</a></p></li>
<li><p><a class="reference external" href="http://iopscience.iop.org/article/10.1086/318656/meta">Analysis of energy spectra with low photon counts via Bayesian posterior simulation</a></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>

          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2017--2021, G.Vianello, J. M. Burgess, N. Di Lalla, N. Omodei, H. Fleischhack.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>